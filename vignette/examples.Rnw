\documentclass[a4paper,justified,openany]{tufte-handout}
\setcounter{secnumdepth}{2}
\usepackage{microtype}
%\usepackage{cite} % Make references as [1-4], not [1,2,3,4]
\usepackage{url}  % Formatting web addresses  
%\usepackage[utf8]{inputenc} %unicode support
%\usepackage{epsfig}
\usepackage{color,hyperref,comment, booktabs}
\urlstyle{rm}
\usepackage{algorithmic, algorithm,amsmath}
\newcommand{\cc}{\texttt}
\newcommand{\xmin}{x_{\min}}

\newcommand{\note}[1]{{\color{red}#1}}

\usepackage{etoolbox}
\makeatletter
\preto{\@verbatim}{\topsep=-20pt \partopsep=0pt }
\makeatother

\date{Last updated: \today}  % if the \date{} command is left out, the current date will be used

\title{The poweRlaw package: Examples}
\author[Colin S. Gillespie]{Colin S. Gillespie}
\date{Last updated: \today}

<<echo=FALSE>>=
library(poweRlaw)
opts_chunk$set(prompt = FALSE,
               fig.path='graphics/', 
               fig.align='center',fig.lp="")
options(width=75)
knit_theme$set(knit_theme$get()[7])

set.seed(1)
source("helper.R")
mypalette(1)
@

\begin{document}
\maketitle
\begin{abstract}
The \verb$poweRlaw$ package provides an easy to use interface for fitting and visualising heavy tailed distributions, including power-laws. The fitting procedure follows the method detailed in Clauset \textit{et al.}\cite{clauset2009}. This vignette works gives examples of the fitting procedure.
\end{abstract}

<<echo=FALSE, results='hide', message=FALSE, warning=FALSE, error=FALSE>>=
if(!file.exists("blackouts.txt"))
  download.file("http://goo.gl/BsqnP", destfile="blackouts.txt")
@

\section{Discrete data: Moby Dick}

The Moby Dick dataset contains the frequency of unique words in the the novel Moby Dick by Herman Melville. The data set can be downloaded from
\begin{center}
\url{http://tuvalu.santafe.edu/~aaronc/powerlaws/data.htm}
\end{center}
\noindent or loaded directly
<<>>=
data(moby)
@
\noindent To fit a discrete power-law to this data\sidenote{The object \texttt{moby} is a simple R vector.}, we use the \texttt{displ} method
<<cache=TRUE>>=
m_pl = displ$new(moby)
@
\noindent The resulting object, \cc{m\_pl}, is a \cc{displ}\sidenote{\cc{displ}: discrete power-law.} object. It also inherits the \cc{discrete\_distribution} class. After creating the \cc{displ} object, a typical first step would be to infer model parameters.\sidenote{When the \cc{displ} object is first created, the default parameter values are \cc{NULL} and $\xmin$ is set to the minimum $x$-value.} We can estimate the lower threshold, via:
<<cache=TRUE>>=
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
@
\noindent For a given value $\xmin$, the scaling parameter is estimated using its MLE:
\[
\hat \alpha \simeq 1 + n \left[\sum_{i=1}^n \log \left(\frac{x_i}{\xmin -0.5}\right)\right]^{-1}
\]
\noindent This yields a threshold estimate of $\xmin=\Sexpr{est$xmin}$ and scaling parameter $\alpha=\Sexpr{signif(est$pars, 3)}$, which matches results found in \citet{clauset2009}. Alternatively, we could perform a parameter scan for each value of $\xmin$:
<<cache=TRUE,results='hide'>>=
estimate_xmin(m_pl, pars=seq(1.5, 2.5, 0.1))
@
\noindent The parameter scan method is more exact, but is slightly slower.

To fit a discrete log-normal distribution, we follows a similar procedure, except we begin by creating a \cc{dislnorm}\sidenote{\cc{dislnorm}: discrete log-normal object}.

<<warning=FALSE, cache=TRUE>>=
m_ln = dislnorm$new(moby)
est = estimate_xmin(m_ln)
@
<<echo=FALSE, cache=TRUE>>=
est_ln = est
est_ln$pars = signif(est_ln$pars, 3)
m_ln$setXmin(est)
m_pois = dispois$new(moby)
est = estimate_xmin(m_pois)
m_pois$setXmin(est)
@

\noindent which yields a lower threshold of $\xmin=\Sexpr{est_ln$xmin}$ and parameters $(\Sexpr{est_ln$pars[1]},$ $\Sexpr{est_ln$pars[2]})$. A similar procedure is applied to fit the Poisson distribution; we create a distribution object using \cc{dispois}, then fit as before.



The data CDF and lines of best fit can be easily plotted:
<<fig.keep='none', cache=TRUE>>=
plot(m_pl)
lines(m_pl, col=2)
lines(m_ln, col=3)
lines(m_pois, col=4)
@
\begin{marginfigure}
\centering
\includegraphics[width=\textwidth]{graphics/v2_figure1a-crop}
\caption{Data CDF of the Moby Dick data set. The fitted power-law (green line), log-normal (red line) and poisson (blue) distributions are also given.}\label{F1a}
\end{marginfigure}
\noindent to obtain figure \ref{F1a}. It clear that the Poisson distribution is not appropriate for this data set. However, the log-normal and power-law distribution both provide reasonable fits to the data.

<<echo=FALSE, cache=TRUE>>=
pdf("graphics/v2_figure1a.pdf", width=4, height=4)
setnicepar(mfrow=c(1,1))
plot(m_pl, xlab="x", ylab="CDF", 
     panel.first=grid(col="grey80"), 
     pch=21, bg=1)

lines(m_pl, col=2, lwd=2)
lines(m_ln, col=3, lwd=2)
lines(m_pois, col=4, lwd=2)

sink = dev.off()
system("pdfcrop graphics/v2_figure1a.pdf")
@

\subsection{Parameter uncertainty}

<<echo=FALSE>>=
data(bootstrap_moby)
bs = bootstrap_moby
@

To get a handle on the uncertainty in the parameter estimates, we use a bootstrapping procedure, via the \texttt{bootstrap} function. This procedure can be applied to any distribution object.\sidenote{For example, \mbox{bootstrap(m\_ln)}.} Furthermore, the bootstrap procedure can utilize multiple CPU cores to speed up inference.\sidenote{The output of this bootstrapping procedure can be obtained via \cc{data(bootstrap\_moby)}.}
<<eval=FALSE, tidy=FALSE>>=
##5000 bootstraps using two cores
bs = bootstrap(m_pl, no_of_sims=5000, threads=2)
@
\noindent By default, the \cc{bootstrap} function will use the maximum likelihood estimate to estimate the parameter and check all values of $\xmin$. When possible $\xmin$ values are large, then it is recommend that the search space is reduced. For example, this function call
<<eval=FALSE>>=
bootstrap(m_pl, xmins = seq(2, 20, 2))
@
\noindent will only calculate the Kolmogorov-Smirnoff statistics at values of $\xmin$ equal to
\[
2, 4, 6, \ldots, 20\;.
\]
A similar argument exists for the parameters.\sidenote{For single parameter models, \cc{pars} should be a vector. For the log-normal distribution, \cc{pars} should be a matrix of values.}

The \cc{bs} object contains three components:
\begin{enumerate}
\item The goodness of fit statistic obtained from the Kolmogorov-Smirnoff test. This value should correspond to the value obtained from the \mbox{\cc{estimate\_xmin}} function.
\item A data frame containing the results for the bootstrap procedure. 
\item The average simulation time, in seconds, for a single bootstrap.
\end{enumerate}
The boostrap results can be explored in a variety way. First we can estimate the standard deviation of the parameter uncertainty, i.e.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{graphics/v2_figure1b-crop}
\caption{Results from the standard bootstrap procedure (for the power-law model) using the Moby Dick data set: \mbox{\texttt{bootstrap(m\_pl)}}. The top row shows the mean estimate of parameters $\xmin$ and $\alpha$. The bottom row shows the estimate of standard deviation for each parameter. The dashed-lines give approximate 95\% confidence intervals.

After 5000 iterations, the standard deviation of $\xmin$ and $\alpha$ is estimated to be 2.1 and 0.03 respectively.}\label{F1b}
\end{figure}
<<>>=
sd(bs$bootstraps[,2])
sd(bs$bootstraps[,3])
@
\noindent Alternatively, we can visualise the results using the \cc{plot} function:
<<fig.keep='none'>>=
plot(bs, trim=0.1)
@
\noindent to obtain figure \ref{F1b}. This top row of graphics in figure \ref{F1b} give a 95\% confidence interval for mean estimate of the parameters. The bottom row of graphics give a 95\% confidence for the standard deviation of the parameters. The parameter \texttt{trim} in the \texttt{plot} function controls the percentage of samples displayed.\sidenote{When \cc{trim=0}, all iterations are displayed.} When \texttt{trim=0.1}, we only display the final 90\% of data. 
\begin{marginfigure}
\centering
\includegraphics[width=\textwidth]{graphics/v2_figure1c-crop}
\caption{Characterising uncertainty in parameter values. (a) $\xmin$ uncertainty (standard deviation 2) (b) $\alpha$ uncertainty (std dev. 0.03)}\label{F1c}
\end{marginfigure}

We can also construct histograms. 
<<fig.keep='none'>>=
hist(bs$bootstraps[,2])
hist(bs$bootstraps[,3]) 
@
\noindent to get figure \ref{F1c}. 

A similar bootstrap analysis can be obtained for the log-normal distribution
<<eval=FALSE>>=
bs1 = bootstrap(m_ln)
@
\noindent in this case we would obtain uncertainty estimates for both of the log-normal parameters.

<<echo=FALSE>>=
pdf("graphics/v2_figure1b.pdf", width=12, height=9)
plot(bs)
sink=dev.off()
system("pdfcrop graphics/v2_figure1b.pdf")

pdf("graphics/v2_figure1c.pdf", width=4, height=8)
setnicepar(mfrow=c(2, 1))
hist(bs$bootstraps[,2], xlab="xmin", ylim=c(0, 1600), 
     xlim=c(0, 20), main=NULL, breaks="fd")
grid()
hist(bs$bootstraps[,3], xlab=expression(alpha), 
     ylim=c(0, 400), xlim=c(1.80, 2.05), main=NULL, breaks="fd")
grid()
sink = dev.off()
system("pdfcrop graphics/v2_figure1c.pdf")
data(bootstrap_p_moby)
bs_p = bootstrap_p_moby
@
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{graphics/v2_figure1d-crop}
\caption{Results from the bootstrap procedure (for the power-law model) using the Moby Dick data set: \mbox{\texttt{bootstrap\_p(m\_pl)}}. The top row shows the mean estimate of parameters $\xmin$,  $\alpha$ and the $p$-value. The bottom row shows the estimate of standard deviation for each parameter. The dashed-lines give approximate 95\% confidence intervals.}\label{F1d}
\end{figure}

\subsection{Testing the power-law hypothesis}

Since it is possible to fit a power-law distribution to \textit{any} data set, it is appropriate to test whether it the observed data set actually follows a power-law. Clauset \textit{et al}, suggest that this hypothesis is tested using a goodness-of-fit test, via a bootstrapping procedure. This test generates a $p$-value that can be used to quantify the plausibility of the hypothesis. If the $p$-value is large, than any difference between the empirical data and the model can be explained with statistical fluctuations. If $p \simeq 0$, then the model does not provide a plausible fit to the data and another distribution may be more appropriate. In this scenario, 
\begin{align*}
&H_0: \mbox{data is generate from a power-law.}\\
&H_1: \mbox{data is not generated from a power-law distribution.}
\end{align*}
\noindent To test these hypothesis, we use the \texttt{bootstrap\_p} function
<<eval=FALSE>>=
bs_p = bootstrap_p(m_pl)
@
\noindent The point estimate of the $p$-value is one of the elements of the \texttt{bs\_p} object\sidenote{Also given is the average time in seconds of a single bootstrap: \mbox{\texttt{bs\_p\$sim\_time} = \Sexpr{signif(bs_p$sim_time, 3)}}.}
<<>>=
bs_p$p
@
\noindent Alternatively we can plot the results

<<fig.keep='none', cache=TRUE>>=
plot(bs_p)
@

\noindent to obtain figure \ref{F1d}. The graph in the top right hand corner gives the cumulative estimate of the $p$-value; the final value of the purple line corresponds to \texttt{bs\_p\$p}. Also given are approximate 95\% confidence intervals. 

<<echo=FALSE, cache=TRUE>>=
pdf("graphics/v2_figure1d.pdf", width=12, height=9)
setnicepar(mfrow=c(2, 1))
plot(bs_p)
sink = dev.off()
system("pdfcrop graphics/v2_figure1d.pdf")
@

\subsection{Investigating the effect in $\xmin$}

The estimate of the scaling parameter, $\alpha$, is typically highly correlated with the threshold limit, $\xmin$. This relationship can be easily investigated with the \cc{poweRlaw} function. First, we create a vector of thresholds to scan
<<>>=
xmins = 1:1500
@
\begin{marginfigure}
\centering
\includegraphics[width=\textwidth]{graphics/v2_figure1e-crop}
\caption{Estimated parameter values conditional on the threshold, $\xmin$. The horizontal line corresponds to $\alpha=1.95$.}\label{F1e}
\end{marginfigure}
\noindent then a vector to store the results
<<>>=
est_scan = 0*xmins
@
\noindent Next, we loop over the $\xmin$ values and estimate the parameter value conditional on the $\xmin$ value:
<<cache=TRUE>>=
for(i in seq_along(xmins)){
  m_pl$setXmin(xmins[i])
  est = estimate_pars(m_pl, pars=seq(1.2, 2.8, 0.01))
  est_scan[i] = est$pars
}
@
\noindent The results are plotted figure \ref{F1e}. For this data set, as the lower threshold increases, so does the point estimate of $\alpha$.


<<echo=FALSE, cache=TRUE>>=
pdf("graphics/v2_figure1e.pdf", width=12, height=9)
setnicepar(mfrow=c(1, 1))
plot(xmins, est_scan, type="l", 
     panel.first=grid(), 
     xlab="xmin", ylab=expression(alpha), 
     ylim=c(1.6, 2.8), col=1)
abline(h=1.95, col=2, lty=2)
sink = dev.off()
system("pdfcrop graphics/v2_figure1e.pdf")
@


\clearpage

\section{Continuous data: electrical blackouts}

The numbers of customers affected in electrical blackouts in the United States between 1984 and 2002.\cite{newman2005} The data set can be downloaded from Clauset's website:\sidenote{\url{http://goo.gl/BsqnP}} 
<<>>=
blackouts = read.table("blackouts.txt")
@
\noindent Although the \texttt{blackouts} data set is actually continuous, since the actual data values are large, it makes sense to treat the data as continuous. Continuous power-law objects take vectors as inputs, so
<<cache=TRUE>>=
m_bl = conpl$new(blackouts$V1)
@
\noindent then we estimate the lower-bound via
<<cache=TRUE>>=
est = estimate_xmin(m_bl)
@
\noindent This gives a point estimate for $\xmin$ of \Sexpr{est$xmin}. We can then update distribution object:
<<cache=TRUE>>=
m_bl$setXmin(est)
@
\noindent We can use the generic plot method
<<fig.keep='none', cache=TRUE>>=
plot(m_bl)
lines(m_bl, col=2, lwd=2)
@
\noindent to get figure \ref{F2a}. To fit a discrete log-normal distribution we follow a similar procedure:
\begin{marginfigure}[5\baselineskip]
\centering
\includegraphics[width=\textwidth]{graphics/v2_figure2a-crop}
\caption{CDF plot of the blackout dataset with line of best fit. Since the minimum value of $x$ is large, we fit a continuous power-law as this is more it efficient. The power-law fit is the green line, the discrete log-normal is the red line.}\label{F2a}
\end{marginfigure}
<<cache=TRUE>>=
m_bl_ln = conlnorm$new(blackouts$V1)
est = estimate_xmin(m_bl_ln)
m_bl_ln$setXmin(est)
@
\noindent and add the line of best fit to the plot via
<<eval=FALSE>>=
lines(m_bl_ln, col=3, lwd=2)
@
\noindent It is clear from figure \ref{F2a} that the log-normal distribution provides a better fit to this data set.


<<echo=FALSE, cache=TRUE>>=
pdf("graphics/v2_figure2a.pdf", width=4, height=4)
setnicepar(mfrow=c(1,1))
plot(m_bl, pch=21, bg=1, 
     panel.first=grid(col="grey80"), 
     xlab="Blackouts", ylab="CDF")
lines(m_bl, col=2, lwd=3)
lines(m_bl_ln, col=3, lwd=3)
sink = dev.off()
system("pdfcrop graphics/v2_figure2a.pdf")
@

\clearpage
\section{Multiple data sets: the American-Indian war}

In a recent paper, \citeauthor{Bohorquez2009} investigated insurgent attacks in Afghanistan, Iraq, Colombia, and Peru.\cite{Bohorquez2009} Each time, the data resembled power laws. \citeauthor{Friedman2013} used the power-law nature of casualties to infer under-reporting in the American-Indian War. Briefly, by fitting a power-law distribution to the observed process, the latent, unobserved casualties can be inferred.\cite{Friedman2013}  

The number of casualties observed in the American-Indian War can via
<<>>=
data(NativeAmerican)
data(USAmerican)
@
\noindent Each data frame, has two columns. The first column is number of casualties recorded, the second the conflict date
<<>>=
head(NativeAmerican, 3)
@
\noindent The records span around one hundred years, 1776 -- 1890. The data is plotted in figure \ref{F3a}. 
\begin{figure}[b]
\centering
\includegraphics[width=0.9\textwidth]{graphics/v2_figure3a-crop}
\caption{Casualty record for the Indian-American war, 1776 -- 1890. Native Americans casualties (purple circles) and US Americans casualties (green triangles). Data taken from \citeauthor{Friedman2013}.}\label{F3a}
\end{figure}

It is straightforward to fit a discrete power-law to this data set. First, we create discrete power-law objects:
<<cache=TRUE>>=
m_na = displ$new(NativeAmerican$Cas)
m_us = displ$new(USAmerican$Cas)
@
\noindent then we estimate $\xmin$ for each data set:
<<cache=TRUE>>=
est_na = estimate_xmin(m_na, pars=seq(1.5, 2.5, 0.001))
est_us = estimate_xmin(m_us, pars=seq(1.5, 2.5, 0.001))
@
\noindent then update the power-law objects
<<cache=TRUE>>=
m_na$setXmin(est_na)
m_us$setXmin(est_us)
@
\noindent The resulting fit lines can be plotted on the same figure
<<fig.keep='none', cache=TRUE>>=
plot(m_na)
lines(m_na)
##Don't create a new plot
##Just store the output
d = plot(m_us, draw=FALSE)
points(d$x, d$y, col=2)
lines(m_us, col=2)
@
\begin{marginfigure}
\includegraphics[width=\textwidth]{graphics/v2_figure3b-crop}
\caption{Plots of the CDFs for the Native American and US American casualties. The lines of best fit are also given.}\label{F3b}
\end{marginfigure}
\noindent The result is given in figure \ref{F3b}. The tails of the distributions seems to follow a power-law. This is consistent with the expectation that smaller-scale engagements are less likely to be recorded. However, for larger scale engagements, it is very likely that a record is made.





<<echo=FALSE, cache=TRUE>>=
pdf("graphics/v2_figure3a.pdf", width=6, height=4)
setnicepar()

plot(NativeAmerican$Date, NativeAmerican$Cas, 
     log="y", pch=21, bg=1, 
     ylim=c(1, 2000), 
     cex=0.5, panel.first=grid(col="grey70"), 
     xlab="Date", ylab="#Casualties")

points(USAmerican$Date, USAmerican$Cas, 
       pch=24, bg=2, cex=0.5)
sink = dev.off()
system("pdfcrop graphics/v2_figure3a.pdf")

pdf("graphics/v2_figure3b.pdf", width=4, height=4)
setnicepar()
plot(m_na, bg=1, pch=21, cex=0.5,  
     panel.first=grid(col="grey70"), 
     xlab="#Casualties", ylab="CDF")
lines(m_na, lwd=2, col=1)

d = plot(m_us, draw=FALSE)
points(d$x, d$y, bg=2, pch=24, cex=0.5)
lines(m_us, lwd=2, col=2)
sink = dev.off()
system("pdfcrop graphics/v2_figure3b.pdf")

@


\bibliography{poweRlaw}
%\bibliographystyle{agsm}
\bibliographystyle{plainnat}
%\bibliographystyle{te}










\end{document}